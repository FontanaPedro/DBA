{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BDA Lab 2: Supervised Machine Learning\n",
    "## Classification and Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "hide_input": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.4\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "import warnings \n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cancer Dataset: Classification\n",
    "\n",
    "First we load the Breast Cancer dataset, which is a Scikit-learn dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cancer.keys():\n",
      " dict_keys(['data', 'target', 'frame', 'target_names', 'DESCR', 'feature_names', 'filename', 'data_module'])\n",
      "Shape of cancer data: (569, 30)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "cancer = load_breast_cancer()\n",
    "print(\"cancer.keys():\\n\", cancer.keys())\n",
    "print(\"Shape of cancer data:\", cancer.data.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'data': array([[1.799e+01, 1.038e+01, 1.228e+02, ..., 2.654e-01, 4.601e-01,\n",
       "         1.189e-01],\n",
       "        [2.057e+01, 1.777e+01, 1.329e+02, ..., 1.860e-01, 2.750e-01,\n",
       "         8.902e-02],\n",
       "        [1.969e+01, 2.125e+01, 1.300e+02, ..., 2.430e-01, 3.613e-01,\n",
       "         8.758e-02],\n",
       "        ...,\n",
       "        [1.660e+01, 2.808e+01, 1.083e+02, ..., 1.418e-01, 2.218e-01,\n",
       "         7.820e-02],\n",
       "        [2.060e+01, 2.933e+01, 1.401e+02, ..., 2.650e-01, 4.087e-01,\n",
       "         1.240e-01],\n",
       "        [7.760e+00, 2.454e+01, 4.792e+01, ..., 0.000e+00, 2.871e-01,\n",
       "         7.039e-02]]),\n",
       " 'target': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0,\n",
       "        1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0,\n",
       "        1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0,\n",
       "        0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1,\n",
       "        1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0,\n",
       "        0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0,\n",
       "        1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1,\n",
       "        1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0,\n",
       "        0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0,\n",
       "        0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0,\n",
       "        1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1,\n",
       "        1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1,\n",
       "        1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,\n",
       "        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,\n",
       "        1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1,\n",
       "        1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1]),\n",
       " 'frame': None,\n",
       " 'target_names': array(['malignant', 'benign'], dtype='<U9'),\n",
       " 'DESCR': '.. _breast_cancer_dataset:\\n\\nBreast cancer wisconsin (diagnostic) dataset\\n--------------------------------------------\\n\\n**Data Set Characteristics:**\\n\\n    :Number of Instances: 569\\n\\n    :Number of Attributes: 30 numeric, predictive attributes and the class\\n\\n    :Attribute Information:\\n        - radius (mean of distances from center to points on the perimeter)\\n        - texture (standard deviation of gray-scale values)\\n        - perimeter\\n        - area\\n        - smoothness (local variation in radius lengths)\\n        - compactness (perimeter^2 / area - 1.0)\\n        - concavity (severity of concave portions of the contour)\\n        - concave points (number of concave portions of the contour)\\n        - symmetry\\n        - fractal dimension (\"coastline approximation\" - 1)\\n\\n        The mean, standard error, and \"worst\" or largest (mean of the three\\n        worst/largest values) of these features were computed for each image,\\n        resulting in 30 features.  For instance, field 0 is Mean Radius, field\\n        10 is Radius SE, field 20 is Worst Radius.\\n\\n        - class:\\n                - WDBC-Malignant\\n                - WDBC-Benign\\n\\n    :Summary Statistics:\\n\\n    ===================================== ====== ======\\n                                           Min    Max\\n    ===================================== ====== ======\\n    radius (mean):                        6.981  28.11\\n    texture (mean):                       9.71   39.28\\n    perimeter (mean):                     43.79  188.5\\n    area (mean):                          143.5  2501.0\\n    smoothness (mean):                    0.053  0.163\\n    compactness (mean):                   0.019  0.345\\n    concavity (mean):                     0.0    0.427\\n    concave points (mean):                0.0    0.201\\n    symmetry (mean):                      0.106  0.304\\n    fractal dimension (mean):             0.05   0.097\\n    radius (standard error):              0.112  2.873\\n    texture (standard error):             0.36   4.885\\n    perimeter (standard error):           0.757  21.98\\n    area (standard error):                6.802  542.2\\n    smoothness (standard error):          0.002  0.031\\n    compactness (standard error):         0.002  0.135\\n    concavity (standard error):           0.0    0.396\\n    concave points (standard error):      0.0    0.053\\n    symmetry (standard error):            0.008  0.079\\n    fractal dimension (standard error):   0.001  0.03\\n    radius (worst):                       7.93   36.04\\n    texture (worst):                      12.02  49.54\\n    perimeter (worst):                    50.41  251.2\\n    area (worst):                         185.2  4254.0\\n    smoothness (worst):                   0.071  0.223\\n    compactness (worst):                  0.027  1.058\\n    concavity (worst):                    0.0    1.252\\n    concave points (worst):               0.0    0.291\\n    symmetry (worst):                     0.156  0.664\\n    fractal dimension (worst):            0.055  0.208\\n    ===================================== ====== ======\\n\\n    :Missing Attribute Values: None\\n\\n    :Class Distribution: 212 - Malignant, 357 - Benign\\n\\n    :Creator:  Dr. William H. Wolberg, W. Nick Street, Olvi L. Mangasarian\\n\\n    :Donor: Nick Street\\n\\n    :Date: November, 1995\\n\\nThis is a copy of UCI ML Breast Cancer Wisconsin (Diagnostic) datasets.\\nhttps://goo.gl/U2Uwz2\\n\\nFeatures are computed from a digitized image of a fine needle\\naspirate (FNA) of a breast mass.  They describe\\ncharacteristics of the cell nuclei present in the image.\\n\\nSeparating plane described above was obtained using\\nMultisurface Method-Tree (MSM-T) [K. P. Bennett, \"Decision Tree\\nConstruction Via Linear Programming.\" Proceedings of the 4th\\nMidwest Artificial Intelligence and Cognitive Science Society,\\npp. 97-101, 1992], a classification method which uses linear\\nprogramming to construct a decision tree.  Relevant features\\nwere selected using an exhaustive search in the space of 1-4\\nfeatures and 1-3 separating planes.\\n\\nThe actual linear program used to obtain the separating plane\\nin the 3-dimensional space is that described in:\\n[K. P. Bennett and O. L. Mangasarian: \"Robust Linear\\nProgramming Discrimination of Two Linearly Inseparable Sets\",\\nOptimization Methods and Software 1, 1992, 23-34].\\n\\nThis database is also available through the UW CS ftp server:\\n\\nftp ftp.cs.wisc.edu\\ncd math-prog/cpo-dataset/machine-learn/WDBC/\\n\\n.. topic:: References\\n\\n   - W.N. Street, W.H. Wolberg and O.L. Mangasarian. Nuclear feature extraction \\n     for breast tumor diagnosis. IS&T/SPIE 1993 International Symposium on \\n     Electronic Imaging: Science and Technology, volume 1905, pages 861-870,\\n     San Jose, CA, 1993.\\n   - O.L. Mangasarian, W.N. Street and W.H. Wolberg. Breast cancer diagnosis and \\n     prognosis via linear programming. Operations Research, 43(4), pages 570-577, \\n     July-August 1995.\\n   - W.H. Wolberg, W.N. Street, and O.L. Mangasarian. Machine learning techniques\\n     to diagnose breast cancer from fine-needle aspirates. Cancer Letters 77 (1994) \\n     163-171.',\n",
       " 'feature_names': array(['mean radius', 'mean texture', 'mean perimeter', 'mean area',\n",
       "        'mean smoothness', 'mean compactness', 'mean concavity',\n",
       "        'mean concave points', 'mean symmetry', 'mean fractal dimension',\n",
       "        'radius error', 'texture error', 'perimeter error', 'area error',\n",
       "        'smoothness error', 'compactness error', 'concavity error',\n",
       "        'concave points error', 'symmetry error',\n",
       "        'fractal dimension error', 'worst radius', 'worst texture',\n",
       "        'worst perimeter', 'worst area', 'worst smoothness',\n",
       "        'worst compactness', 'worst concavity', 'worst concave points',\n",
       "        'worst symmetry', 'worst fractal dimension'], dtype='<U23'),\n",
       " 'filename': 'breast_cancer.csv',\n",
       " 'data_module': 'sklearn.datasets.data'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_breast_cancer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1\n",
    "cancer.target is a array consisting of 0's and 1's. We'd like to know the distribution -- print this, using np.bincount. Print the names of the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([212, 357])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.bincount(cancer.target)  #amounts of 0´s and 1´s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['mean radius' 'mean texture' 'mean perimeter' 'mean area'\n",
      " 'mean smoothness' 'mean compactness' 'mean concavity'\n",
      " 'mean concave points' 'mean symmetry' 'mean fractal dimension'\n",
      " 'radius error' 'texture error' 'perimeter error' 'area error'\n",
      " 'smoothness error' 'compactness error' 'concavity error'\n",
      " 'concave points error' 'symmetry error' 'fractal dimension error'\n",
      " 'worst radius' 'worst texture' 'worst perimeter' 'worst area'\n",
      " 'worst smoothness' 'worst compactness' 'worst concavity'\n",
      " 'worst concave points' 'worst symmetry' 'worst fractal dimension']\n"
     ]
    }
   ],
   "source": [
    "print(cancer.feature_names)  #one of the keys is the features = feautures for my ML model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30,)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cancer.feature_names.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.799e+01 1.038e+01 1.228e+02 ... 2.654e-01 4.601e-01 1.189e-01]\n",
      " [2.057e+01 1.777e+01 1.329e+02 ... 1.860e-01 2.750e-01 8.902e-02]\n",
      " [1.969e+01 2.125e+01 1.300e+02 ... 2.430e-01 3.613e-01 8.758e-02]\n",
      " ...\n",
      " [1.660e+01 2.808e+01 1.083e+02 ... 1.418e-01 2.218e-01 7.820e-02]\n",
      " [2.060e+01 2.933e+01 1.401e+02 ... 2.650e-01 4.087e-01 1.240e-01]\n",
      " [7.760e+00 2.454e+01 4.792e+01 ... 0.000e+00 2.871e-01 7.039e-02]]\n"
     ]
    }
   ],
   "source": [
    "print(cancer.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2\n",
    "Create a test train split using cancer.data and cancer.target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cancer.target will be my laballed data y --> I want to know if its 0 or 1\n",
    "#cancer.data is my sam\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(cancer.data, cancer.target, stratify=cancer.target, random_state=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3\n",
    "Create a KNeighborsClassifier. Use the fit method with the train data. Then score the model on both the train and test data, and print the results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier()"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn = KNeighborsClassifier()\n",
    "knn.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of KNN n-5, on the training set: 0.946\n",
      "Accuracy of KNN n-5, on the test set: 0.916\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print('Accuracy of KNN n-5, on the training set: {:.3f}'.format(knn.score(X_train, y_train)))\n",
    "print('Accuracy of KNN n-5, on the test set: {:.3f}'.format(knn.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4\n",
    "Create a loop that builds kNN classifiers with either distance or uniform weighting, with numbers of neighbors varying between 1 and 20. What is the best combination? Produce a list consisting of test accuracy, training accuracy, number of neighbors and weighting choice. The list should be sorted by test accuracy.\n",
    "\n",
    "<hr>\n",
    "\n",
    "<i>Hints: </i>\n",
    "<br>\n",
    "<i>\n",
    "Create two dicts: training_accuracy and test_accuracy. Make two loops: an outer loop ranging over number of neighbors, and an inner loop ranging over weighting choice. Then build the model with the current options and create a key unique to that option (eg. by concatenating the two options). Then you can store the current results in the the training and test dicts, using the current key. When you're done with the loops, you print out the sorted results of the two dicts.</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test  -   Train    -  k, weighting\n",
      "\n",
      "0.888  -  0.977   -   2 uniform\n",
      "0.902  -  1.000   -   1 distance\n",
      "0.902  -  1.000   -   1 uniform\n",
      "0.902  -  1.000   -   2 distance\n",
      "0.916  -  1.000   -   4 distance\n",
      "0.916  -  0.934   -   9 uniform\n",
      "0.916  -  1.000   -   10 distance\n",
      "0.916  -  0.939   -   10 uniform\n",
      "0.923  -  1.000   -   3 distance\n",
      "0.923  -  0.958   -   3 uniform\n",
      "0.923  -  0.955   -   4 uniform\n",
      "0.923  -  0.948   -   5 uniform\n",
      "0.923  -  1.000   -   8 distance\n",
      "0.923  -  1.000   -   9 distance\n",
      "0.930  -  1.000   -   5 distance\n",
      "0.930  -  1.000   -   6 distance\n",
      "0.930  -  1.000   -   7 distance\n",
      "0.930  -  0.944   -   7 uniform\n",
      "0.930  -  0.941   -   8 uniform\n",
      "0.937  -  0.946   -   6 uniform\n"
     ]
    }
   ],
   "source": [
    "#Stratify is making sure that the split is than 75%/25% by label as well and specifying the label column\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(cancer.data, cancer.target, stratify=cancer.target, random_state=66)\n",
    "\n",
    "# Create two lists for training and test accuracies\n",
    "training_accuracy = {}\n",
    "test_accuracy = {}\n",
    "\n",
    "# Define a range of 1 to 10 (included) neighbors to be tested\n",
    "neighbors_settings = range(1,11)\n",
    "w = [\"distance\", \"uniform\"]\n",
    "\n",
    "# Loop with the KNN through the different number of neighbors to determine the most appropriate (best)\n",
    "for n_neighbors in neighbors_settings:\n",
    "    for ws in w:\n",
    "        \n",
    "        key = str(n_neighbors) + \" \" + ws   # I convert n_neighbors into str otherwise I cannot add it to ws\n",
    "                                            # The key is the combination of the number of k + the weight combinations, e.g. \"2 Uniform\" or \"2 Distance\"\n",
    "                                            # Then is that case you can search for your accuracy by naming the uniform of distance (just by searching thr \"key\")\n",
    "    \n",
    "        knn = KNeighborsClassifier(n_neighbors=n_neighbors, weights=ws)\n",
    "        knn.fit(X_train, y_train)\n",
    "        training_accuracy[key]=\"{:.3f}\".format(knn.score(X_train, y_train))\n",
    "        #training_accuracy.append(clf.score(X_train, y_train))  \n",
    "        test_accuracy[key]=\"{:.3f}\".format(knn.score(X_test, y_test))\n",
    "\n",
    "        \n",
    "#res= sorted(zip(training_accuracy, test_accuracy))\n",
    "print (\"Test  -   Train    -  k, weighting\\n\")\n",
    "\n",
    "\n",
    "# With test_accuracy.GET i just select the value of test_accuracy\n",
    "\n",
    "#Reverse : Use the reverse parameter in sorted() to sort the dictionary in reverse order, \n",
    "#based on the second argument --> in this case test was the second argument, thats why the lowest test_accuracy is 1st\n",
    "\n",
    "#The value of the key parameter should be a function (or other callable) that takes a single argument and \n",
    "#returns a key to use for sorting purposes. This technique is fast because the key function is called \n",
    "#exactly once for each input record.\n",
    "\n",
    "\n",
    "for key in sorted(test_accuracy, key = test_accuracy.get, reverse = False):\n",
    "    \n",
    "    print(test_accuracy[key],\" - \", training_accuracy[key],\"  -  \", key)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear models for classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 5\n",
    "Produce a LogisticRegression classifier with default settings for the cancer dataset. Print the training and test accuracy.<br>\n",
    "<i>(You may get a warning, TOTAL NO. of ITERATIONS REACHED LIMIT. You can ignore that. Or you can try larger settings for the optional parameter, max_iter, to remove the warning. You may see improved scores as a result!)</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(cancer.data, cancer.target, stratify=cancer.target, random_state = 0)\n",
    "\n",
    "log_reg = LogisticRegression()  #here I am creating a classifier\n",
    "log_reg.fit(X_train, y_train) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on the training subset: 0.958\n",
      "Accuracy on the test subset: 0.937\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy on the training subset: {:.3f}'.format(log_reg.score(X_train, y_train)))\n",
    "print('Accuracy on the test subset: {:.3f}'.format(log_reg.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 6\n",
    "Print the three features with the highest coefficients -- these are the features which the model most strongly links with the target value of 1, i.e., <i>benign</i>. Print the three features with the lowest coefficients -- these are the features which the model most strongly links with the target value 0, i.e., <i>malignant</i>. On each line, print the feature name, followed by its coefficient value.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('mean radius', 1.5401326895947085)\n",
      "('worst radius', 1.4431551414281087)\n",
      "('texture error', 0.5980304133214931)\n",
      "('perimeter error', 0.47130761409091976)\n",
      "('mean texture', 0.14825821884843635)\n",
      "('mean perimeter', 0.10838088991509104)\n",
      "('radius error', 0.03831100309254111)\n",
      "('smoothness error', -0.005887078627145201)\n",
      "('fractal dimension error', -0.006717993286983086)\n",
      "('mean area', -0.0071515141095640136)\n",
      "('symmetry error', -0.02162683705571555)\n",
      "('mean fractal dimension', -0.022202032610368568)\n",
      "('concave points error', -0.022325780772632146)\n",
      "('worst area', -0.022421602968474407)\n",
      "('compactness error', -0.06484907721542935)\n",
      "('mean smoothness', -0.06935069577776512)\n",
      "('mean symmetry', -0.09000480496750345)\n",
      "('worst fractal dimension', -0.0916629456565422)\n",
      "('concavity error', -0.09232314271652398)\n",
      "('worst smoothness', -0.11737342420026399)\n",
      "('area error', -0.11775112916014886)\n",
      "('mean concave points', -0.1768256861136574)\n",
      "('worst perimeter', -0.2274198242701929)\n",
      "('worst symmetry', -0.26269538203229686)\n",
      "('mean compactness', -0.2985915061072213)\n",
      "('worst texture', -0.3292413520444602)\n",
      "('worst concave points', -0.3421762303096131)\n",
      "('mean concavity', -0.412423747163664)\n",
      "('worst compactness', -0.8969185229888859)\n",
      "('worst concavity', -1.0972363532985538)\n"
     ]
    }
   ],
   "source": [
    "sorted_coef= sorted(list(zip(list(cancer.feature_names), log_reg.coef_[0])),\n",
    "                    key=lambda e: e[1], reverse=True)\n",
    "\n",
    "                    \n",
    "for i in sorted_coef:\n",
    "    print(i)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The top 3 features are:  [('mean radius', 1.5401326895947085), ('worst radius', 1.4431551414281087), ('texture error', 0.5980304133214931)]\n",
      "\n",
      "The Lowest 3 features are:  [('mean concavity', -0.412423747163664), ('worst compactness', -0.8969185229888859), ('worst concavity', -1.0972363532985538)]\n"
     ]
    }
   ],
   "source": [
    "print(\"The top 3 features are: \", sorted_coef[0:3])\n",
    "print(\"\\nThe Lowest 3 features are: \", sorted_coef[-3:])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function np.argpartition(coefs, k) will return an array that starts with the indices of the smallest n-k elements in coefs and ends with the indices of the largest k elements in coefs. Since it does not perform a full sort, it is more efficient than doing a full sort of the array (note that using -3 in the function is the same as using len(coefs)-3). If you don't need the efficiency you could also replace that row with top_three = np.argsort(coefs)[-3:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 7\n",
    "Print the 3 features that are the <i>least</i> informative about our target value (Hint: you should use the absolute value of coefficients.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('smoothness error', -0.005887078627145201), ('fractal dimension error', -0.006717993286983086), ('mean area', -0.0071515141095640136)]\n"
     ]
    }
   ],
   "source": [
    "sorted_coef_1= list(zip(list(cancer.feature_names), log_reg.coef_[0]))  # I could have run it without this since its rearranging what it on top of it\n",
    "sorted_coef_abs= sorted(sorted_coef_1, key=lambda x:abs(x[1]))\n",
    "\n",
    "# Lambda itera por cada elemento de la lista y aplica la funcion después de x a cada elemento. \n",
    "# y tomo [1] porque las listas operan con posicion empezando en cero\n",
    "# y considerando la lista e arriba quiero aplicar abs a log_reg.coef\n",
    "# luego abajo tomo the [0:3] que significa los primeros 3 elementos de la nueva lista de abs values\n",
    "# cuando pongo key=lambda es basicamente algo fijo que viene con sorted functions. es como poner weights in KNN classifier\n",
    "#\n",
    "\n",
    "\n",
    "least_informative_features= sorted_coef_abs[0:3]\n",
    "\n",
    "print(least_informative_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('smoothness error', -0.005887078627145201), ('fractal dimension error', -0.006717993286983086), ('mean area', -0.0071515141095640136), ('symmetry error', -0.02162683705571555), ('mean fractal dimension', -0.022202032610368568), ('concave points error', -0.022325780772632146), ('worst area', -0.022421602968474407), ('radius error', 0.03831100309254111), ('compactness error', -0.06484907721542935), ('mean smoothness', -0.06935069577776512), ('mean symmetry', -0.09000480496750345), ('worst fractal dimension', -0.0916629456565422), ('concavity error', -0.09232314271652398), ('mean perimeter', 0.10838088991509104), ('worst smoothness', -0.11737342420026399), ('area error', -0.11775112916014886), ('mean texture', 0.14825821884843635), ('mean concave points', -0.1768256861136574), ('worst perimeter', -0.2274198242701929), ('worst symmetry', -0.26269538203229686), ('mean compactness', -0.2985915061072213), ('worst texture', -0.3292413520444602), ('worst concave points', -0.3421762303096131), ('mean concavity', -0.412423747163664), ('perimeter error', 0.47130761409091976), ('texture error', 0.5980304133214931), ('worst compactness', -0.8969185229888859), ('worst concavity', -1.0972363532985538), ('worst radius', 1.4431551414281087), ('mean radius', 1.5401326895947085)]\n"
     ]
    }
   ],
   "source": [
    "print(sorted_coef_abs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 8\n",
    "Try different values of C for a Logistic Regression model with the same data. Give results sorted by test accuracy. Each line of the output should include test accuracy, training accuracy, and C value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The test accuracy is:  0.895 with an accuracy on the training set of:  0.934 Using C value of:  0.0001\n",
      "The test accuracy is:  0.937 with an accuracy on the training set of:  0.953 Using C value of:  0.001\n",
      "The test accuracy is:  0.923 with an accuracy on the training set of:  0.939 Using C value of:  0.01\n",
      "The test accuracy is:  0.930 with an accuracy on the training set of:  0.946 Using C value of:  0.1\n",
      "The test accuracy is:  0.937 with an accuracy on the training set of:  0.958 Using C value of:  1\n",
      "The test accuracy is:  0.930 with an accuracy on the training set of:  0.960 Using C value of:  10\n",
      "The test accuracy is:  0.951 with an accuracy on the training set of:  0.965 Using C value of:  100\n",
      "The test accuracy is:  0.930 with an accuracy on the training set of:  0.965 Using C value of:  1000\n"
     ]
    }
   ],
   "source": [
    "c=[0.0001,0.001,0.01,0.1,1,10,100,1000]\n",
    "\n",
    "for i in c:\n",
    "    log_reg=LogisticRegression(C = i)\n",
    "    log_reg.fit(X_train, y_train) \n",
    "    test_acc=\"{:.3f}\".format(log_reg.score(X_test,y_test))\n",
    "    test_tr=\"{:.3f}\".format(log_reg.score(X_train,y_train))\n",
    "    print(\"The test accuracy is: \", test_acc, \"with an accuracy on the training set of: \", test_tr, \"Using C value of: \", i)\n",
    "\n",
    "\n",
    "# There is an example of how to print it in the PPT, I need to rewrite it \n",
    "\n",
    "#print (\"    Test\",  \"   Tran\",   \"C val   2\")\n",
    "#for r in "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Regression</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape: (20640, 8)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "\n",
    "## the code below about ssl is because of a possible error in data access\n",
    "## if you get the error you should uncomment it\n",
    "#import ssl\n",
    "\n",
    "#try:\n",
    "#    _create_unverified_https_context = ssl._create_unverified_context\n",
    "#except AttributeError:\n",
    "#    pass\n",
    "#else:\n",
    "#    ssl._create_default_https_context = _create_unverified_https_context\n",
    "\n",
    "\n",
    "housing = fetch_california_housing()\n",
    "\n",
    "print(\"Data shape:\", housing.data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['data', 'target', 'frame', 'target_names', 'feature_names', 'DESCR'])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "housing.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. _california_housing_dataset:\n",
      "\n",
      "California Housing dataset\n",
      "--------------------------\n",
      "\n",
      "**Data Set Characteristics:**\n",
      "\n",
      "    :Number of Instances: 20640\n",
      "\n",
      "    :Number of Attributes: 8 numeric, predictive attributes and the target\n",
      "\n",
      "    :Attribute Information:\n",
      "        - MedInc        median income in block group\n",
      "        - HouseAge      median house age in block group\n",
      "        - AveRooms      average number of rooms per household\n",
      "        - AveBedrms     average number of bedrooms per household\n",
      "        - Population    block group population\n",
      "        - AveOccup      average number of household members\n",
      "        - Latitude      block group latitude\n",
      "        - Longitude     block group longitude\n",
      "\n",
      "    :Missing Attribute Values: None\n",
      "\n",
      "This dataset was obtained from the StatLib repository.\n",
      "https://www.dcc.fc.up.pt/~ltorgo/Regression/cal_housing.html\n",
      "\n",
      "The target variable is the median house value for California districts,\n",
      "expressed in hundreds of thousands of dollars ($100,000).\n",
      "\n",
      "This dataset was derived from the 1990 U.S. census, using one row per census\n",
      "block group. A block group is the smallest geographical unit for which the U.S.\n",
      "Census Bureau publishes sample data (a block group typically has a population\n",
      "of 600 to 3,000 people).\n",
      "\n",
      "An household is a group of people residing within a home. Since the average\n",
      "number of rooms and bedrooms in this dataset are provided per household, these\n",
      "columns \n"
     ]
    }
   ],
   "source": [
    "print(housing.DESCR[:1400])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 9\n",
    "Build a LinearRegression model using the housing dataset, and print the score on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 10\n",
    "Print the 3 features with the highest coefficients, those with the lowest, and those closest to zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "latex_metadata": {
   "author": "Andreas C. M\\\"ller",
   "title": "Machine Learning with Python"
  },
  "vscode": {
   "interpreter": {
    "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
